{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ddfb98",
   "metadata": {},
   "source": [
    "# DeepHisCoM Simulation Notebook\n",
    "\n",
    "이 노트북은 Python 스크립트 대신 Jupyter Notebook 형태로, `simulation/` 폴더의 시뮬레이션 결과와 대사체 정보를 이용하여 DeepHisCoM 모델을 학습합니다.\n",
    "\n",
    "- 사전 조건: `181_metabolite_clinical.csv`, `layerinfo.csv`, `annot.csv`, 그리고 `simulation/` 폴더 내부의 CSV 파일들이 작업 디렉토리에 위치해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97024743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정 섹션\n",
    "seed = 100\n",
    "perm_times = 1000\n",
    "gpu_num = 0\n",
    "# 사용할 시나리오 파일 이름\n",
    "scenario_file = 'linear_beta_0.1.csv'  # 예시: simulation/linear_beta_0.1.csv\n",
    "# 학습 하이퍼파라미터\n",
    "batch_size = 0\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "leakyrelu_const = 0.2\n",
    "activation = 'leakyrelu'\n",
    "loss_type = 'BCELoss'\n",
    "reg_type = 'l1'\n",
    "reg_const_path_disease = 0\n",
    "reg_const_bio_path = 0\n",
    "stop_type = 0\n",
    "divide_rate = 0.2\n",
    "count_lim = 5\n",
    "cov = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 이동 및 데이터 로딩\n",
    "os.chdir(os.getcwd())\n",
    "\n",
    "# 대사체 데이터\n",
    "df_meta = pd.read_csv('181_metabolite_clinical.csv', index_col=0)\n",
    "metabolite = df_meta.iloc[:, 14:].reset_index(drop=True)\n",
    "\n",
    "# 시뮬레이션 결과\n",
    "sim_df = pd.read_csv(os.path.join('simulation', scenario_file))\n",
    "outcome_col = [c for c in sim_df.columns if c.startswith('y_')][0]\n",
    "sim_df = sim_df[[outcome_col]].rename(columns={outcome_col: 'phenotype'})\n",
    "\n",
    "# 통합 데이터프레임 생성\n",
    "train_base = pd.concat([metabolite, sim_df], axis=1)\n",
    "train_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리: 로그 변환 + 표준화\n",
    "eps = 1e-6\n",
    "X_log = np.log(metabolite.values + eps)\n",
    "X_scaled = StandardScaler().fit_transform(X_log)\n",
    "df_pre = pd.DataFrame(X_scaled, columns=metabolite.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 매핑 설정\n",
    "mapping = {\n",
    "    'map00400': ['Phe', 'Trp', 'Tyr'],\n",
    "    'map00860': ['Glu', 'Gly', 'Thr']\n",
    "}\n",
    "X1 = df_pre[mapping['map00400']].values\n",
    "X2 = df_pre[mapping['map00860']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layerinfo 및 annot 로딩\n",
    "layerinfo = pd.read_csv('layerinfo.csv')\n",
    "node_num = layerinfo['node_num'].tolist()\n",
    "layer_num = layerinfo['layer_num'].tolist()\n",
    "annot = pd.read_csv('annot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29341f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그룹별 변수 수 계산\n",
    "from collections import OrderedDict\n",
    "groupunique = list(OrderedDict.fromkeys(annot['group']))\n",
    "nvar = [(annot['group']==g).sum() for g in groupunique]\n",
    "cov_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76992c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 및 데이터셋 클래스\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, DATA, inputlength):\n",
    "        self.x_data = DATA[:, :-1]\n",
    "        self.y_data = DATA[:, -1].reshape(-1,1)\n",
    "    def __len__(self): return len(self.x_data)\n",
    "    def __getitem__(self, idx): return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "act_fn_by_name = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"identity\": nn.Identity, \"leakyrelu\": nn.LeakyReLU}\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear): m.weight.data.fill_(0.01)\n",
    "\n",
    "class pathwayblock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_num):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if hidden_dim == 0:\n",
    "            layers += [nn.Linear(input_dim,1,bias=False), act_fn, nn.Dropout(dropout_rate)]\n",
    "        else:\n",
    "            layers += [nn.Linear(input_dim,hidden_dim,bias=False), act_fn, nn.Dropout(dropout_rate)]\n",
    "            for _ in range(layer_num-1): layers += [nn.Linear(hidden_dim,hidden_dim,bias=False), act_fn, nn.Dropout(dropout_rate)]\n",
    "            layers += [nn.Linear(hidden_dim,1,bias=False), act_fn, nn.Dropout(dropout_rate)]\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        self.block.apply(init_weights)\n",
    "    def forward(self, x): return self.block(x)\n",
    "\n",
    "class DeepHisCoM(nn.Module):\n",
    "    def __init__(self, nvar, width, layer, covariate):\n",
    "        super().__init__()\n",
    "        self.pathway_nn = nn.ModuleList([pathwayblock(nvar[i], width[i], layer[i]) for i in range(len(nvar))])\n",
    "        self.bn_path = nn.BatchNorm1d(len(nvar))\n",
    "        self.fc_path_disease = nn.Linear(len(nvar)+covariate,1)\n",
    "        self.fc_path_disease.weight.data.fill_(0)\n",
    "        self.fc_path_disease.bias.data.fill_(0.001)\n",
    "    def forward(self, x):\n",
    "        splits = np.cumsum([0]+nvar+[cov_num])\n",
    "        p = [self.pathway_nn[i](x[:,splits[i]:splits[i+1]]) for i in range(len(nvar))]\n",
    "        path = torch.cat(p,1)\n",
    "        path = self.bn_path(path)\n",
    "        path = path/torch.norm(path,2)\n",
    "        x_cat = torch.cat([path, x[:,splits[-1]:]],1)\n",
    "        x_cat = nn.Dropout(dropout_rate)(x_cat)\n",
    "        return torch.sigmoid(self.fc_path_disease(x_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcafd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프: 단일 permutation 예시\n",
    "# 전체 반복(permutation)을 수행하려면 for문으로 감싸세요\n",
    "np.random.seed(seed)\n",
    "train = train_base.copy()\n",
    "phen = train['phenotype']\n",
    "train = (train - train.mean())/train.std()\n",
    "train['phenotype'] = phen\n",
    "\n",
    "tensor = torch.from_numpy(train.values).float()\n",
    "dataset = CustomDataset(tensor, tensor.shape[1])\n",
    "loader = DataLoader(dataset, batch_size=(batch_size or len(dataset)), shuffle=True)\n",
    "\n",
    "model = DeepHisCoM(nvar, node_num, layer_num, cov_num)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss() if loss_type=='BCELoss' else nn.MSELoss()\n",
    "\n",
    "# 한 epoch 훈련 예시\n",
    "model.train()\n",
    "for x_batch, y_batch in loader:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_batch)\n",
    "    loss = criterion(y_pred.squeeze(), y_batch.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print('Training complete')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
